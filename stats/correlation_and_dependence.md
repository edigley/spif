Correlation and dependence

In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. 
In the broadest sense correlation is any statistical association, 
    though in common usage it most often refers to how close two variables are to having a linear relationship with each other. 
    
Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, 
    and the correlation between the demand for a limited supply product and its price. 

Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.
For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather.
In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling.

However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation). 

Formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence.

In informal parlance, correlation is synonymous with dependence.

However, when used in a technical sense, correlation refers to any of several specific types of relationship between mean values.

There are several correlation coefficients, often denoted p or r, measuring the degree of correlation.

The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables 
(which may be present even when one variable is a nonlinear function of the other).

Other correlation coefficients have been developed to be more robust than the Pearson correlation
    â€“ that is, more sensitive to nonlinear relationships.

Mutual information can also be applied to measure dependence between two variables. 