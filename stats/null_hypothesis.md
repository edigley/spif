Null hypothesis

In inferential statistics, the null hypothesis is a general statement or default position that 
    there is no relationship between two measured phenomena, or no association among groups.

Testing (accepting, approving, rejecting, or disproving) the null hypothesis—and thus concluding that 
    there are or are not grounds for believing that there is a relationship between two phenomena 
        (e.g. that a potential treatment has a measurable effect)—
    is a central task in the modern practice of science; the field of statistics gives precise criteria for rejecting a null hypothesis. 

The null hypothesis is generally assumed to be true until evidence indicates otherwise. 


The concept of a null hypothesis is used differently in two approaches to statistical inference. In the significance testing approach of Ronald Fisher, a null hypothesis is rejected if the observed data are significantly unlikely to have occurred if the null hypothesis were true. In this case the null hypothesis is rejected and an alternative hypothesis is accepted in its place. If the data are consistent with the null hypothesis, then the null hypothesis is not rejected. In neither case is the null hypothesis or its alternative proven; the null hypothesis is tested with data and a decision is made based on how likely or unlikely the data are. This is analogous to the legal principle of presumption of innocence, in which a suspect or defendant is assumed to be innocent (null is not rejected) until proven guilty (null is rejected) beyond a reasonable doubt (to a statistically significant degree). 

In the hypothesis testing approach of Jerzy Neyman and Egon Pearson, a null hypothesis is contrasted with an alternative hypothesis and the two hypotheses are distinguished on the basis of data, with certain error rates. It is used in formulating answers in researches. 

Statistical inference can be done without a null hypothesis, by specifying a statistical model corresponding to each candidate hypothesis and using model selection techniques to choose the most appropriate model.[2] (The most common selection techniques are based on either Akaike information criterion or Bayes factor.) 